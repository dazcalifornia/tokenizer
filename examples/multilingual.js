/**
 * @file multilingual.js
 * @description Multilingual tokenization examples
 */

const Tokenizer = require("../src/tokenizer");

// Create a universal tokenizer
const tokenizer = new Tokenizer();

console.log("\n========== MULTILINGUAL TOKENIZATION ==========\n");

// Helper function to test tokenization
function testLanguage(name, code, text) {
  console.log(`\n----- ${name} (${code}) -----`);
  console.log(`Original: ${text}`);
  const tokens = tokenizer.tokenize(text);
  console.log(`Tokens (${tokens.length}): ${JSON.stringify(tokens)}`);
  return tokens;
}

// Test various languages

// 1. English
testLanguage(
  "English",
  "en",
  "Natural language processing is a fascinating field of AI research.",
);

// 2. Thai
testLanguage("Thai", "th", "à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¸à¸¹à¸”à¹ƒà¸™à¸›à¸£à¸°à¹€à¸—à¸¨à¹„à¸—à¸¢");

// 3. Chinese
testLanguage("Chinese", "zh", "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸ");

// 4. Japanese
testLanguage("Japanese", "ja", "è‡ªç„¶è¨€èªå‡¦ç†ã¯äººå·¥çŸ¥èƒ½ã®ä¸€åˆ†é‡ã§ã™");

// 5. Korean
testLanguage("Korean", "ko", "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µ ì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤");

// 6. Arabic
testLanguage(
  "Arabic",
  "ar",
  "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù‡ÙŠ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ",
);

// 7. Hebrew
testLanguage("Hebrew", "he", "×¢×™×‘×•×“ ×©×¤×” ×˜×‘×¢×™×ª ×”×•× ×ª×—×•× ×©×œ ×‘×™× ×” ××œ××›×•×ª×™×ª");

// 8. Russian
testLanguage(
  "Russian",
  "ru",
  "ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ° - ÑÑ‚Ğ¾ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑŒ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚Ğ°",
);

// 9. Hindi
testLanguage(
  "Hindi",
  "hi",
  "à¤ªà¥à¤°à¤¾à¤•à¥ƒà¤¤à¤¿à¤• à¤­à¤¾à¤·à¤¾ à¤ªà¥à¤°à¤¸à¤‚à¤¸à¥à¤•à¤°à¤£ à¤•à¥ƒà¤¤à¥à¤°à¤¿à¤® à¤¬à¥à¤¦à¥à¤§à¤¿à¤®à¤¤à¥à¤¤à¤¾ à¤•à¤¾ à¤à¤• à¤•à¥à¤·à¥‡à¤¤à¥à¤° à¤¹à¥ˆ",
);

// 10. Vietnamese
testLanguage(
  "Vietnamese",
  "vi",
  "Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn lÃ  má»™t lÄ©nh vá»±c cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o",
);

// 11. Turkish
testLanguage("Turkish", "tr", "DoÄŸal dil iÅŸleme, yapay zekanÄ±n bir alanÄ±dÄ±r");

// 12. Greek
testLanguage(
  "Greek",
  "el",
  "Î— ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Ï†Ï…ÏƒÎ¹ÎºÎ®Ï‚ Î³Î»ÏÏƒÏƒÎ±Ï‚ ÎµÎ¯Î½Î±Î¹ Î­Î½Î±Ï‚ Ï„Î¿Î¼Î­Î±Ï‚ Ï„Î·Ï‚ Ï„ÎµÏ‡Î½Î·Ï„Î®Ï‚ Î½Î¿Î·Î¼Î¿ÏƒÏÎ½Î·Ï‚",
);

// Mixed script text
console.log("\n----- Mixed Scripts -----");
const mixedText =
  "This is English, ã“ã‚Œã¯æ—¥æœ¬èªã§ã™, à¸™à¸µà¹ˆà¸„à¸·à¸­à¸ à¸²à¸©à¸²à¹„à¸—à¸¢, è¿™æ˜¯ä¸­æ–‡, Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©";
console.log(`Original: ${mixedText}`);
const mixedTokens = tokenizer.tokenize(mixedText);
console.log(`Tokens (${mixedTokens.length}): ${JSON.stringify(mixedTokens)}`);

// Social media style text with emojis and hashtags
console.log("\n----- Social Media Text -----");
const socialText =
  "Just landed in Bangkok! à¸à¸£à¸¸à¸‡à¹€à¸—à¸à¸¯à¸ªà¸§à¸¢à¸¡à¸²à¸ ğŸ˜ #Thailand #vacation";
console.log(`Original: ${socialText}`);
const socialTokens = tokenizer.tokenize(socialText);
console.log(`Tokens (${socialTokens.length}): ${JSON.stringify(socialTokens)}`);

// Technical text with numbers and symbols
console.log("\n----- Technical Text -----");
const technicalText =
  "The model achieved 97.8% accuracy on the test set (p < 0.001)";
console.log(`Original: ${technicalText}`);
const technicalTokens = tokenizer.tokenize(technicalText);
console.log(
  `Tokens (${technicalTokens.length}): ${JSON.stringify(technicalTokens)}`,
);

// Medical text with abbreviations
console.log("\n----- Medical Text -----");
const medicalText = "Pt presents with SOB, HR 110, BP 140/90, O2 sat 92% on RA";
console.log(`Original: ${medicalText}`);
const medicalTokens = tokenizer.tokenize(medicalText);
console.log(
  `Tokens (${medicalTokens.length}): ${JSON.stringify(medicalTokens)}`,
);

console.log("\n========== END OF MULTILINGUAL EXAMPLES ==========\n");
